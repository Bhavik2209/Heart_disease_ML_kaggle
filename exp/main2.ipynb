{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HEART DISEASE PREDICTION — POST-EDA MODELING PIPELINE\n",
    "# =============================================================================\n",
    "# Dataset   : Heart Disease (Clinical Features)\n",
    "# Target    : Heart Disease (Absence = 0, Presence = 1)\n",
    "# Objective : Predict coronary artery disease probability (ROC-AUC)\n",
    "# Models    : LightGBM · XGBoost · CatBoost · Stacking Ensemble\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 1 — DATA LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 1: DATA LOADING & PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df    = pd.read_csv(\"/content/train.csv\")\n",
    "test  = pd.read_csv(\"/content/test.csv\")\n",
    "\n",
    "# ── Target encoding ──────────────────────────────────────────────────────────\n",
    "df[\"Heart Disease\"] = df[\"Heart Disease\"].map({\"Absence\": 0, \"Presence\": 1})\n",
    "\n",
    "# ── Drop rows where target is NaN (1 row) ────────────────────────────────────\n",
    "df.dropna(subset=[\"Heart Disease\"], inplace=True)\n",
    "df[\"Heart Disease\"] = df[\"Heart Disease\"].astype(int)\n",
    "\n",
    "# ── Drop ID column ────────────────────────────────────────────────────────────\n",
    "df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "print(f\"Train shape : {df.shape}\")\n",
    "print(f\"Test  shape : {test.shape}\")\n",
    "print(f\"Target distribution:\\n{df['Heart Disease'].value_counts()}\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 2 — FEATURE GROUPS & PREPROCESSOR DEFINITIONS\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 2: FEATURE GROUPS & PREPROCESSORS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ── Feature groups (based on EDA findings) ───────────────────────────────────\n",
    "numeric_features = [\n",
    "    \"Age\", \"BP\", \"Cholesterol\", \"Max HR\", \"ST depression\"\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    \"Sex\", \"FBS over 120\", \"Exercise angina\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Chest pain type\",   # nominal — 4 unordered categories\n",
    "    \"EKG results\",       # nominal — values 0/1/2 are labels, not a scale\n",
    "    \"Slope of ST\",       # nominal — non-linear jump between levels (EDA)\n",
    "    \"Thallium\"           # nominal — non-consecutive codes 3/6/7\n",
    "]\n",
    "\n",
    "ordinal_features = [\n",
    "    \"Number of vessels fluro\"   # ordinal — monotonic risk increase 0→3\n",
    "]\n",
    "\n",
    "TARGET = \"Heart Disease\"\n",
    "\n",
    "# ── Preprocessor for tree-based models (no scaling needed) ───────────────────\n",
    "preprocessor_tree = ColumnTransformer(transformers=[\n",
    "    (\"num\", \"passthrough\",                                  numeric_features),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"),         categorical_features),\n",
    "    (\"bin\", \"passthrough\",                                  binary_features),\n",
    "    (\"ord\", \"passthrough\",                                  ordinal_features),\n",
    "])\n",
    "\n",
    "# ── Preprocessor for linear models (StandardScaler + OneHot) ─────────────────\n",
    "preprocessor_linear = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(),                               numeric_features),\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), categorical_features),\n",
    "    (\"bin\", \"passthrough\",                                  binary_features),\n",
    "    (\"ord\", \"passthrough\",                                  ordinal_features),\n",
    "])\n",
    "\n",
    "print(\"Feature groups defined.\")\n",
    "print(f\"  Numeric     : {len(numeric_features)} features\")\n",
    "print(f\"  Categorical : {len(categorical_features)} features (OneHot encoded)\")\n",
    "print(f\"  Binary      : {len(binary_features)} features (pass-through)\")\n",
    "print(f\"  Ordinal     : {len(ordinal_features)} feature  (pass-through)\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 3 — CROSS-VALIDATION SETUP\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 3: CROSS-VALIDATION SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "X_test    = test.drop(columns=[\"id\"])\n",
    "test_ids  = test[\"id\"]\n",
    "\n",
    "SKF = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Strategy : 5-Fold Stratified K-Fold (maintains class ratio per fold)\")\n",
    "print(f\"Train size: {len(X):,}  |  Positive rate: {y.mean():.4f}\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 4 — MODEL 1: LightGBM (Optuna-Tuned Parameters)\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 4: MODEL 1 — LightGBM Classifier\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  Best hyperparameters sourced from Optuna search (30 trials).\")\n",
    "print(\"  Scoring metric: ROC-AUC  |  Validation: 5-Fold OOF\\n\")\n",
    "\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators       = 1661,\n",
    "    learning_rate      = 0.012503626241860565,\n",
    "    num_leaves         = 31,\n",
    "    max_depth          = 6,\n",
    "    min_child_samples  = 43,\n",
    "    subsample          = 0.643072395692159,\n",
    "    colsample_bytree   = 0.7211161958467457,\n",
    "    reg_alpha          = 1.0504857541588257,\n",
    "    reg_lambda         = 0.11946226125639381,\n",
    "    random_state       = 42,\n",
    "    n_jobs             = -1,\n",
    "    verbosity          = -1,\n",
    ")\n",
    "\n",
    "lgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor_tree),\n",
    "    (\"model\",        lgb_model),\n",
    "])\n",
    "\n",
    "# OOF evaluation\n",
    "oof_lgb = np.zeros(len(X))\n",
    "for fold, (tr_idx, val_idx) in enumerate(SKF.split(X, y)):\n",
    "    X_tr, X_val = X.iloc[tr_idx],  X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx],  y.iloc[val_idx]\n",
    "    lgb_pipeline.fit(X_tr, y_tr)\n",
    "    oof_lgb[val_idx] = lgb_pipeline.predict_proba(X_val)[:, 1]\n",
    "    print(f\"  Fold {fold+1} AUC: {roc_auc_score(y_val, oof_lgb[val_idx]):.5f}\")\n",
    "\n",
    "lgb_oof_auc = roc_auc_score(y, oof_lgb)\n",
    "print(f\"\\n  LightGBM OOF ROC-AUC : {lgb_oof_auc:.5f}\\n\")\n",
    "\n",
    "# Full refit on all training data\n",
    "lgb_pipeline.fit(X, y)\n",
    "lgb_test_preds = lgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 5 — MODEL 2: XGBoost (Optuna-Tuned Parameters)\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 5: MODEL 2 — XGBoost Classifier\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  Best hyperparameters sourced from Optuna search (Trial 0).\")\n",
    "print(\"  Scoring metric: ROC-AUC  |  Validation: 5-Fold OOF\\n\")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators     = 972,\n",
    "    learning_rate    = 0.08233334476657686,\n",
    "    max_depth        = 3,\n",
    "    subsample        = 0.6967792979720865,\n",
    "    colsample_bytree = 0.7773146292728021,\n",
    "    reg_alpha        = 1.911349598671315,\n",
    "    reg_lambda       = 0.6194119678307304,\n",
    "    tree_method      = \"hist\",\n",
    "    eval_metric      = \"logloss\",\n",
    "    random_state     = 42,\n",
    "    n_jobs           = -1,\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor_tree),\n",
    "    (\"model\",        xgb_model),\n",
    "])\n",
    "\n",
    "# OOF evaluation\n",
    "oof_xgb = np.zeros(len(X))\n",
    "for fold, (tr_idx, val_idx) in enumerate(SKF.split(X, y)):\n",
    "    X_tr, X_val = X.iloc[tr_idx],  X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx],  y.iloc[val_idx]\n",
    "    xgb_pipeline.fit(X_tr, y_tr)\n",
    "    oof_xgb[val_idx] = xgb_pipeline.predict_proba(X_val)[:, 1]\n",
    "    print(f\"  Fold {fold+1} AUC: {roc_auc_score(y_val, oof_xgb[val_idx]):.5f}\")\n",
    "\n",
    "xgb_oof_auc = roc_auc_score(y, oof_xgb)\n",
    "print(f\"\\n  XGBoost OOF ROC-AUC  : {xgb_oof_auc:.5f}\\n\")\n",
    "\n",
    "# Full refit\n",
    "xgb_pipeline.fit(X, y)\n",
    "xgb_test_preds = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 6 — MODEL 3: CatBoost (Optuna-Tuned Parameters)\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 6: MODEL 3 — CatBoost Classifier\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  Best hyperparameters sourced from Optuna search (Trial 3).\")\n",
    "print(\"  CatBoost handles categoricals natively — no OneHot encoding needed.\")\n",
    "print(\"  Scoring metric: ROC-AUC  |  Validation: 5-Fold OOF\\n\")\n",
    "\n",
    "# CatBoost requires categorical columns as strings\n",
    "cat_cols_cb = [\n",
    "    \"Sex\", \"Chest pain type\", \"FBS over 120\", \"EKG results\",\n",
    "    \"Exercise angina\", \"Slope of ST\", \"Number of vessels fluro\", \"Thallium\"\n",
    "]\n",
    "\n",
    "def prepare_catboost_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.copy()\n",
    "    for col in cat_cols_cb:\n",
    "        data[col] = data[col].astype(str)\n",
    "    return data\n",
    "\n",
    "X_cb      = prepare_catboost_data(X)\n",
    "X_test_cb = prepare_catboost_data(X_test)\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations          = 2443,\n",
    "    learning_rate       = 0.028617286398439353,\n",
    "    depth               = 6,\n",
    "    l2_leaf_reg         = 3.5313325975665264,\n",
    "    bagging_temperature = 0.5274409717782269,\n",
    "    random_strength     = 0.03843459373261249,\n",
    "    random_seed         = 42,\n",
    "    verbose             = 0,\n",
    ")\n",
    "\n",
    "# OOF evaluation (using CatBoost Pool API for proper categorical handling)\n",
    "oof_cat = np.zeros(len(X_cb))\n",
    "for fold, (tr_idx, val_idx) in enumerate(SKF.split(X_cb, y)):\n",
    "    X_tr, X_val = X_cb.iloc[tr_idx],  X_cb.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx],     y.iloc[val_idx]\n",
    "\n",
    "    train_pool = Pool(X_tr,  y_tr,  cat_features=cat_cols_cb)\n",
    "    val_pool   = Pool(X_val, y_val, cat_features=cat_cols_cb)\n",
    "\n",
    "    fold_model = CatBoostClassifier(\n",
    "        iterations          = 2443,\n",
    "        learning_rate       = 0.028617286398439353,\n",
    "        depth               = 6,\n",
    "        l2_leaf_reg         = 3.5313325975665264,\n",
    "        bagging_temperature = 0.5274409717782269,\n",
    "        random_strength     = 0.03843459373261249,\n",
    "        random_seed         = 42,\n",
    "        verbose             = 0,\n",
    "    )\n",
    "    fold_model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=200)\n",
    "    oof_cat[val_idx] = fold_model.predict_proba(val_pool)[:, 1]\n",
    "    print(f\"  Fold {fold+1} AUC: {roc_auc_score(y_val, oof_cat[val_idx]):.5f}\")\n",
    "\n",
    "cat_oof_auc = roc_auc_score(y, oof_cat)\n",
    "print(f\"\\n  CatBoost OOF ROC-AUC : {cat_oof_auc:.5f}\\n\")\n",
    "\n",
    "# Full refit\n",
    "full_pool       = Pool(X_cb, y, cat_features=cat_cols_cb)\n",
    "test_pool       = Pool(X_test_cb, cat_features=cat_cols_cb)\n",
    "cat_model.fit(full_pool)\n",
    "cat_test_preds  = cat_model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 7 — MODEL COMPARISON SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 7: INDIVIDUAL MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_summary = pd.DataFrame({\n",
    "    \"Model\"       : [\"LightGBM\", \"XGBoost\", \"CatBoost\"],\n",
    "    \"OOF ROC-AUC\" : [lgb_oof_auc, xgb_oof_auc, cat_oof_auc],\n",
    "})\n",
    "model_summary = model_summary.sort_values(\"OOF ROC-AUC\", ascending=False).reset_index(drop=True)\n",
    "print(model_summary.to_string(index=False))\n",
    "print()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 8 — BLENDING: WEIGHTED AVERAGE ENSEMBLE\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 8: BLENDING — Weighted Average Ensemble (LGB + XGB + CAT)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  Strategy: Sweep blend weights and pick best OOF AUC combination.\")\n",
    "print(\"  Only two weights needed — third is derived (sums to 1.0).\\n\")\n",
    "\n",
    "best_blend_auc    = 0\n",
    "best_blend_weights = (0.33, 0.33, 0.34)\n",
    "\n",
    "for w_lgb in np.arange(0.1, 0.7, 0.1):\n",
    "    for w_xgb in np.arange(0.1, 0.7, 0.1):\n",
    "        w_cat = round(1.0 - w_lgb - w_xgb, 2)\n",
    "        if w_cat < 0.05 or w_cat > 0.8:\n",
    "            continue\n",
    "        blend = w_lgb * oof_lgb + w_xgb * oof_xgb + w_cat * oof_cat\n",
    "        auc   = roc_auc_score(y, blend)\n",
    "        if auc > best_blend_auc:\n",
    "            best_blend_auc     = auc\n",
    "            best_blend_weights = (w_lgb, w_xgb, w_cat)\n",
    "\n",
    "w_lgb, w_xgb, w_cat = best_blend_weights\n",
    "print(f\"  Best blend weights → LGB: {w_lgb:.1f} | XGB: {w_xgb:.1f} | CAT: {w_cat:.2f}\")\n",
    "print(f\"  Blended OOF ROC-AUC     : {best_blend_auc:.5f}\\n\")\n",
    "\n",
    "blend_test_preds = w_lgb * lgb_test_preds + w_xgb * xgb_test_preds + w_cat * cat_test_preds\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 9 — STACKING: META-LEARNER ENSEMBLE\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 9: STACKING ENSEMBLE — HistGradientBoosting Meta-Learner\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  Base learners : LightGBM · XGBoost · CatBoost (OOF predictions)\")\n",
    "print(\"  Meta-learner  : HistGradientBoostingClassifier\")\n",
    "print(\"  Note: Meta-learner is trained on OOF predictions to avoid leakage.\\n\")\n",
    "\n",
    "# Stack OOF predictions as features for meta-learner\n",
    "stacked_train = np.column_stack([oof_lgb, oof_xgb, oof_cat])\n",
    "stacked_test  = np.column_stack([lgb_test_preds, xgb_test_preds, cat_test_preds])\n",
    "\n",
    "meta_model = HistGradientBoostingClassifier(\n",
    "    max_depth     = 3,\n",
    "    learning_rate = 0.05,\n",
    "    max_iter      = 300,\n",
    "    random_state  = 42,\n",
    ")\n",
    "meta_model.fit(stacked_train, y)\n",
    "\n",
    "stacked_oof_preds  = meta_model.predict_proba(stacked_train)[:, 1]\n",
    "stacked_oof_auc    = roc_auc_score(y, stacked_oof_preds)\n",
    "stacked_test_preds = meta_model.predict_proba(stacked_test)[:, 1]\n",
    "\n",
    "print(f\"  Stacked OOF ROC-AUC : {stacked_oof_auc:.5f}\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 10 — FINAL RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 10: FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Approach\"    : [\"LightGBM\", \"XGBoost\", \"CatBoost\",\n",
    "                     \"Blend (LGB+XGB+CAT)\", \"Stacking Ensemble\"],\n",
    "    \"OOF ROC-AUC\" : [lgb_oof_auc, xgb_oof_auc, cat_oof_auc,\n",
    "                     best_blend_auc, stacked_oof_auc],\n",
    "})\n",
    "results_df = results_df.sort_values(\"OOF ROC-AUC\", ascending=False).reset_index(drop=True)\n",
    "results_df[\"OOF ROC-AUC\"] = results_df[\"OOF ROC-AUC\"].map(\"{:.5f}\".format)\n",
    "print(results_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Identify best approach\n",
    "best_idx      = results_df[\"OOF ROC-AUC\"].astype(float).idxmax()\n",
    "best_approach = results_df.loc[best_idx, \"Approach\"]\n",
    "best_auc      = results_df.loc[best_idx, \"OOF ROC-AUC\"]\n",
    "print(f\"  ✓ Best approach : {best_approach}  (OOF AUC = {best_auc})\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECTION 11 — SUBMISSION FILES\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 11: GENERATING SUBMISSION FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def save_submission(preds: np.ndarray, ids: pd.Series, filename: str) -> None:\n",
    "    sub = pd.DataFrame({\"id\": ids, \"Heart Disease\": preds})\n",
    "    sub.to_csv(filename, index=False)\n",
    "    print(f\"  Saved: {filename}  ({len(sub):,} rows)\")\n",
    "\n",
    "save_submission(lgb_test_preds,     test_ids, \"submission_lightgbm.csv\")\n",
    "save_submission(xgb_test_preds,     test_ids, \"submission_xgboost.csv\")\n",
    "save_submission(cat_test_preds,     test_ids, \"submission_catboost.csv\")\n",
    "save_submission(blend_test_preds,   test_ids, \"submission_blend.csv\")\n",
    "save_submission(stacked_test_preds, test_ids, \"submission_stacked.csv\")\n",
    "\n",
    "print(\"\\nAll submissions generated successfully.\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
